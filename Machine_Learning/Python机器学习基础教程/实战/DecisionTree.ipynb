{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-16T00:39:17.895699100Z",
     "start_time": "2024-04-16T00:39:17.855052400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    直接使用sklearn中决策树模型，sklearn自带的红酒数据集作为数据源。\\n'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    直接使用sklearn中决策树模型，sklearn自带的红酒数据集作为数据源。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n    重要参数:\\n            criterion : {\"gini\", \"entropy\"}, default=\"gini\"\\n            输入\"entropy\"，使用信息熵(Entropy)\\n            输入\"gini\"，使用基尼系数(Gini Impurity)\\n    决策树找出最佳节点和最佳分枝方法，而衡量这个\"最佳\"的标准(criterion)叫做\"不纯度\"。\\n    一般地，不纯度越低，决策树对训练集的拟合越好。\\n    目前的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。\\n    \\n            random_state : int or RandomState, default=None\\n            random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显。\\n            如果int, random_state是随机数生成器使用的种子;\\n            如果输入RandomState实例，则random_state为随机数生成器;\\n            如果None，随机数生成器就是通过\"np.random\"使用一个RandomState实例\\n            \\n            splitter : {\"best\", \"random\"}, default=\"best\"\\n            输入\"best\"选择最好的分裂\\n            输入\"random\"选择最好的随机分割\\n            用于在每个节点选择分割的策略。\\n            策略是\"best\"，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝(重要性可以通过属性feature_importances_查看)。\\n            策略是\"random\"时，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合，从而降低了过拟合。\\n            \\n            max_depth : int, default=None\\n            输入\"None\" ，则展开节点直到所有的叶子都是纯净的，即不纯度为0，或者直到所有的叶子都含有少于min_samples_split样本。\\n            输入int ，限制树的最大深度，超过设定深度的树枝全部剪掉。\\n            这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。\\n            决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。\\n            \\n            min_samples_split : int or float, default=2\\n            一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则 分枝就不会发生。\\n            输入int，min_samples_split作为最小值。\\n            如果是float，那么min_samples_split 是一个分数和 ceil(min_samples_split * n_samples)是最小值每次分割的样本数。\\n            \\n            min_samples_leaf : int or float, default=1\\n            一个叶节点上所需的最小样本数。即一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或分枝会朝着满足每个子节点都包含min_samples_leaf 个样本的方向去发生。\\n\\n            如果是int，那么考虑min_samples_leaf作为最小值。\\n            如果是float，那么min_samples_leaf 是一个分数和ceil(min_samples_leaf * n_samples)是最小值每个节点的样本数。\\n            任何深度上的分裂点，只有在每个左右分支上的训练样本超过min_samples_leaf，才会被纳入考虑范围。\\n            这会产生平滑模型的效果，尤其是在回归树中使用。\\n            这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。\\n            如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。\\n            同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。\\n            对于类别不多的分类问题，等于1通常就是最佳选择。\\n            \\n            max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\\n            限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，即是直接限制可以使用的特征数量而强行使决策树停下的参数。\\n            是用来限制高维度数据的过拟合的剪枝参数。\\n\\n            如果是int，那么考虑每个分割的max_features特性。\\n            如果是float，输入的浮点数为比例，每个分枝考虑的特征数目是 int(max_features * n_features)，输入模型的数据集的特征个数n_features 。\\n            输入\"auto\"，那么max_features=sqrt(n_features)\\n            输入\"sqrt\"，那么max_features=sqrt(n_features)\\n            输入\"log2\"，那么max_features=log2(n_features)\\n            输入\"None\"，那么max_features=n_feature。\\n            在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。\\n            如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。\\n            \\n            min_impurity_decrease : float, default=0.0\\n            限制信息增益的大小，信息增益小于设定数值的分枝不会发生。当一个节点的分枝后引起的不纯度降低大于或等于 min_impurity_decrease 中输入的数值，则这个分枝则会被保留，不会被剪枝。\\n            \\n            带权重的不纯度下降表示为：\\n            ![带权重的不纯度下降](./不纯度.png)\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    sklearn.tree.DecisionTreeClassifier\n",
    "    sklearn.tree.DecisionTreeClassifier(criterion=’gini’, \n",
    "                                    splitter=’best’, \n",
    "                                    max_depth=None,\n",
    "                                    min_samples_split=2, \n",
    "                                    min_samples_leaf=1, \n",
    "                                    min_weight_fraction_leaf=0.0, \n",
    "                                    max_features=None, \n",
    "                                    random_state=None, \n",
    "                                    max_leaf_nodes=None, \n",
    "                                    min_impurity_decrease=0.0, \n",
    "                                    min_impurity_split=None, \n",
    "                                    class_weight=None, \n",
    "                                    presort=False)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    重要参数:\n",
    "            criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
    "            输入\"entropy\"，使用信息熵(Entropy)\n",
    "            输入\"gini\"，使用基尼系数(Gini Impurity)\n",
    "    决策树找出最佳节点和最佳分枝方法，而衡量这个\"最佳\"的标准(criterion)叫做\"不纯度\"。\n",
    "    一般地，不纯度越低，决策树对训练集的拟合越好。\n",
    "    目前的决策树算法在分枝方法上的核心大多是围绕在对某个不纯度相关指标的最优化上。\n",
    "    \n",
    "            random_state : int or RandomState, default=None\n",
    "            random_state用来设置分枝中的随机模式的参数，默认None，在高维度时随机性会表现更明显。\n",
    "            如果int, random_state是随机数生成器使用的种子;\n",
    "            如果输入RandomState实例，则random_state为随机数生成器;\n",
    "            如果None，随机数生成器就是通过\"np.random\"使用一个RandomState实例\n",
    "            \n",
    "            splitter : {\"best\", \"random\"}, default=\"best\"\n",
    "            输入\"best\"选择最好的分裂\n",
    "            输入\"random\"选择最好的随机分割\n",
    "            用于在每个节点选择分割的策略。\n",
    "            策略是\"best\"，决策树在分枝时虽然随机，但是还是会优先选择更重要的特征进行分枝(重要性可以通过属性feature_importances_查看)。\n",
    "            策略是\"random\"时，决策树在分枝时会更加随机，树会因为含有更多的不必要信息而更深更大，并因这些不必要信息而降低对训练集的拟合，从而降低了过拟合。\n",
    "            \n",
    "            max_depth : int, default=None\n",
    "            输入\"None\" ，则展开节点直到所有的叶子都是纯净的，即不纯度为0，或者直到所有的叶子都含有少于min_samples_split样本。\n",
    "            输入int ，限制树的最大深度，超过设定深度的树枝全部剪掉。\n",
    "            这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。\n",
    "            决策树多生长一层，对样本量的需求会增加一倍，所以限制树深度能够有效地限制过拟合。\n",
    "            \n",
    "            min_samples_split : int or float, default=2\n",
    "            一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则 分枝就不会发生。\n",
    "            输入int，min_samples_split作为最小值。\n",
    "            如果是float，那么min_samples_split 是一个分数和 ceil(min_samples_split * n_samples)是最小值每次分割的样本数。\n",
    "            \n",
    "            min_samples_leaf : int or float, default=1\n",
    "            一个叶节点上所需的最小样本数。即一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或分枝会朝着满足每个子节点都包含min_samples_leaf 个样本的方向去发生。\n",
    "\n",
    "            如果是int，那么考虑min_samples_leaf作为最小值。\n",
    "            如果是float，那么min_samples_leaf 是一个分数和ceil(min_samples_leaf * n_samples)是最小值每个节点的样本数。\n",
    "            任何深度上的分裂点，只有在每个左右分支上的训练样本超过min_samples_leaf，才会被纳入考虑范围。\n",
    "            这会产生平滑模型的效果，尤其是在回归树中使用。\n",
    "            这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。\n",
    "            如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。\n",
    "            同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。\n",
    "            对于类别不多的分类问题，等于1通常就是最佳选择。\n",
    "            \n",
    "            max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
    "            限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃，即是直接限制可以使用的特征数量而强行使决策树停下的参数。\n",
    "            是用来限制高维度数据的过拟合的剪枝参数。\n",
    "\n",
    "            如果是int，那么考虑每个分割的max_features特性。\n",
    "            如果是float，输入的浮点数为比例，每个分枝考虑的特征数目是 int(max_features * n_features)，输入模型的数据集的特征个数n_features 。\n",
    "            输入\"auto\"，那么max_features=sqrt(n_features)\n",
    "            输入\"sqrt\"，那么max_features=sqrt(n_features)\n",
    "            输入\"log2\"，那么max_features=log2(n_features)\n",
    "            输入\"None\"，那么max_features=n_feature。\n",
    "            在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型学习不足。\n",
    "            如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。\n",
    "            \n",
    "            min_impurity_decrease : float, default=0.0\n",
    "            限制信息增益的大小，信息增益小于设定数值的分枝不会发生。当一个节点的分枝后引起的不纯度降低大于或等于 min_impurity_decrease 中输入的数值，则这个分枝则会被保留，不会被剪枝。\n",
    "            \n",
    "            带权重的不纯度下降表示为：\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:03:33.109766400Z",
     "start_time": "2024-04-16T01:03:33.087081800Z"
    }
   },
   "id": "678e835b99cb6c63",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "![带权重的不纯度下降](./不纯度.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac3ff33a64546453"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    其中N为样本总数，Nt为样本总数在当前节点上，NtL是样本的数量，NtR是叶子节点的样本数。\n",
    "    如果 \"sample_weight\" 在fit接口中有值， N、Nt、NtL和NtR 所有引用加权和，而非单纯的样本数量。\n",
    "    \n",
    "    class_weight : dict, list of dict or \"balanced\", default=None\n",
    "    以{class_label: weight} 的形式与类关联的权重。\n",
    "    如果 None列表中字典的权重的顺序需要与各个y在标签数据集中的排列顺序相同。\n",
    "    注意，对于多输出问题(包括多标签问题)，定义的权重必须具体到每个标签下的每个类，其中类是字典键值对中的键。\n",
    "    例如，对于四类多标签分类，权重应是({0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1})。\n",
    "    而不是[{1:1}，{2:5}，{3:1}，{4:1}]。\n",
    "    \n",
    "    如果使用\"balanced\"模式，将会使用y的值来自动调整与输入数据中的类频率成反比的权重，如n_samples / (n_classes * np.bincount(y))\n",
    "    对于多输出，将y的每一列的权重相乘。\n",
    "    \n",
    "    注意，如果指定了sample_weight，这些权重将通过fit接口与sample_weight 相乘。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d28a25a27f37d7c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    其他参数\n",
    "        min_weight_fraction_leaf : float, default=0.0\n",
    "        一个叶节点要存在所需要的权重占输入模型的数据集的总权重的比例。\n",
    "        总权重由fit接口中的sample_weight参数确定，当sample_weight时None时，默认所有样本权重相同。\n",
    "        max_leaf_nodes : int, default=None\n",
    "        最大叶子节点数量，在最佳分枝下，以max_leaf_nodes为限制来生长树。默认没有节点数量的限制。\n",
    "        presort : deprecated, default='deprecated'\n",
    "        是否预先分配数据以加快拟合中最佳分枝的发现。在大型数据集上使用默认设置决策树时，将这个参数设置为True 可能会延长训练过程，降低训练速度。\n",
    "        当使用较小数据集或限制树的深度时，设置这个参数为True 可能会加快训练速度。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6e5278e676fe717"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    属性\n",
    "        classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
    "        输出一个数组(array)，或者一个数组的列表(list)，结构为标签的数目(n_classes)输出所有标签。\n",
    "        feature_importances_ : ndarray of shape (n_features,)\n",
    "        输出一个数组，结构为特征的数目(n_features)。返回每个特征的重要性，一般是这个特征多次分枝中产生的信息增益的综合，亦称作\"基尼重要性\"（Gini importance）\n",
    "        max_features_ : int\n",
    "        输出参数max_features的推断值\n",
    "        n_classes_ : int or list of int\n",
    "        标签类别的数据\n",
    "        n_features_ : int\n",
    "        在训练模型fit时使用的特征个数\n",
    "        n_outputs_ : int\n",
    "        在训练模型fit时使用的结果个数\n",
    "        tree_ : Tree\n",
    "        输出一个可以导出建好的决策树结果的端口，可以通过这个端口访问树的结构和低级属性，包括但不限于查看：\n",
    "        二叉树的结构\n",
    "        每个节点的深度以及它是否时叶子\n",
    "        使用decision_path方法的示例到达的节点\n",
    "        用apply接口取样出的叶子\n",
    "        用于预测样本的规则\n",
    "        一组样本共享的决策路径\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af674787d48112d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "        重要接口\n",
    "            fit(X, y[, sample_weight, check_input, …])\n",
    "            训练模型的接口，其中X代表训练样本的特征，y表示目标数据，即标签。X和y都必须是类数组结构，一般使用ndarray导入。\n",
    "            sample_weitht 是fit的参数，用来为样本标签设置权重，输入的格式是一个和测试集样本量一致长度的数字数组，数组中所带的数字表示每个样本所占的权重，数组中数字的综合代表整个测试集的权重总数。\n",
    "            返回训练完毕的模型。\n",
    "            predict(X[, check_input])\n",
    "            预测所提供的测试集X中样本点的标签，这里的测试集X必须和fit中提供的测试集结构一致。\n",
    "            返回模型预测的测试样本的标签或回归值。\n",
    "            predict_proba(X[, check_input])\n",
    "            预测所提供的测试集X中样本点归属于各个标签的概率。\n",
    "            返回测试集中每个样本点所对应的每个标签的概率，各个标签按辞典顺序排列。预测的类概率是叶子节点中相同类的样本的分数。\n",
    "            score(X, y[, sample_weight])\n",
    "            用给定测试数据和标签的平均准确度作为模型的评分标准，分数越高模型越好。其中X是测试集，y是测试集的真实标签。sample_weight是score的参数，用法与fit的参数一致。\n",
    "            返回给定决策树数据和标签的平均准确度，在多标签分类中，这个指标是子集精度。\n",
    "            apply(X[, check_input])\n",
    "            输入测试集或样本点，返回每个样本点被分到的叶子节点的索引。\n",
    "            check_input是接口apply的参数，输入布尔值，默认True，通常不使用。\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e3698372825401"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 导包\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn .tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:13:33.289740Z",
     "start_time": "2024-04-16T01:13:33.282070500Z"
    }
   },
   "id": "2bba57c63ecee9f8",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(178, 13)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入并探索数据集\n",
    "wine = load_wine()\n",
    "wine.data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:14:35.414241Z",
     "start_time": "2024-04-16T01:14:35.385101100Z"
    }
   },
   "id": "2bd3e15032786857",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:14:44.577582700Z",
     "start_time": "2024-04-16T01:14:44.568067800Z"
    }
   },
   "id": "a9e0b7d4d29562ab",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:14:47.081366300Z",
     "start_time": "2024-04-16T01:14:47.068982600Z"
    }
   },
   "id": "1f3db0616a9a3b0e",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['alcohol',\n 'malic_acid',\n 'ash',\n 'alcalinity_of_ash',\n 'magnesium',\n 'total_phenols',\n 'flavanoids',\n 'nonflavanoid_phenols',\n 'proanthocyanins',\n 'color_intensity',\n 'hue',\n 'od280/od315_of_diluted_wines',\n 'proline']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.feature_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:15:28.436087800Z",
     "start_time": "2024-04-16T01:15:28.427334500Z"
    }
   },
   "id": "7c322df51aa627d4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n0      14.23        1.71  2.43               15.6      127.0           2.80   \n1      13.20        1.78  2.14               11.2      100.0           2.65   \n2      13.16        2.36  2.67               18.6      101.0           2.80   \n3      14.37        1.95  2.50               16.8      113.0           3.85   \n4      13.24        2.59  2.87               21.0      118.0           2.80   \n..       ...         ...   ...                ...        ...            ...   \n173    13.71        5.65  2.45               20.5       95.0           1.68   \n174    13.40        3.91  2.48               23.0      102.0           1.80   \n175    13.27        4.28  2.26               20.0      120.0           1.59   \n176    13.17        2.59  2.37               20.0      120.0           1.65   \n177    14.13        4.10  2.74               24.5       96.0           2.05   \n\n     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n0          3.06                  0.28             2.29             5.64  1.04   \n1          2.76                  0.26             1.28             4.38  1.05   \n2          3.24                  0.30             2.81             5.68  1.03   \n3          3.49                  0.24             2.18             7.80  0.86   \n4          2.69                  0.39             1.82             4.32  1.04   \n..          ...                   ...              ...              ...   ...   \n173        0.61                  0.52             1.06             7.70  0.64   \n174        0.75                  0.43             1.41             7.30  0.70   \n175        0.69                  0.43             1.35            10.20  0.59   \n176        0.68                  0.53             1.46             9.30  0.60   \n177        0.76                  0.56             1.35             9.20  0.61   \n\n     od280/od315_of_diluted_wines  proline  0  \n0                            3.92   1065.0  0  \n1                            3.40   1050.0  0  \n2                            3.17   1185.0  0  \n3                            3.45   1480.0  0  \n4                            2.93    735.0  0  \n..                            ...      ... ..  \n173                          1.74    740.0  2  \n174                          1.56    750.0  2  \n175                          1.56    835.0  2  \n176                          1.62    840.0  2  \n177                          1.60    560.0  2  \n\n[178 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alcohol</th>\n      <th>malic_acid</th>\n      <th>ash</th>\n      <th>alcalinity_of_ash</th>\n      <th>magnesium</th>\n      <th>total_phenols</th>\n      <th>flavanoids</th>\n      <th>nonflavanoid_phenols</th>\n      <th>proanthocyanins</th>\n      <th>color_intensity</th>\n      <th>hue</th>\n      <th>od280/od315_of_diluted_wines</th>\n      <th>proline</th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.23</td>\n      <td>1.71</td>\n      <td>2.43</td>\n      <td>15.6</td>\n      <td>127.0</td>\n      <td>2.80</td>\n      <td>3.06</td>\n      <td>0.28</td>\n      <td>2.29</td>\n      <td>5.64</td>\n      <td>1.04</td>\n      <td>3.92</td>\n      <td>1065.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.20</td>\n      <td>1.78</td>\n      <td>2.14</td>\n      <td>11.2</td>\n      <td>100.0</td>\n      <td>2.65</td>\n      <td>2.76</td>\n      <td>0.26</td>\n      <td>1.28</td>\n      <td>4.38</td>\n      <td>1.05</td>\n      <td>3.40</td>\n      <td>1050.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13.16</td>\n      <td>2.36</td>\n      <td>2.67</td>\n      <td>18.6</td>\n      <td>101.0</td>\n      <td>2.80</td>\n      <td>3.24</td>\n      <td>0.30</td>\n      <td>2.81</td>\n      <td>5.68</td>\n      <td>1.03</td>\n      <td>3.17</td>\n      <td>1185.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.37</td>\n      <td>1.95</td>\n      <td>2.50</td>\n      <td>16.8</td>\n      <td>113.0</td>\n      <td>3.85</td>\n      <td>3.49</td>\n      <td>0.24</td>\n      <td>2.18</td>\n      <td>7.80</td>\n      <td>0.86</td>\n      <td>3.45</td>\n      <td>1480.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13.24</td>\n      <td>2.59</td>\n      <td>2.87</td>\n      <td>21.0</td>\n      <td>118.0</td>\n      <td>2.80</td>\n      <td>2.69</td>\n      <td>0.39</td>\n      <td>1.82</td>\n      <td>4.32</td>\n      <td>1.04</td>\n      <td>2.93</td>\n      <td>735.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>13.71</td>\n      <td>5.65</td>\n      <td>2.45</td>\n      <td>20.5</td>\n      <td>95.0</td>\n      <td>1.68</td>\n      <td>0.61</td>\n      <td>0.52</td>\n      <td>1.06</td>\n      <td>7.70</td>\n      <td>0.64</td>\n      <td>1.74</td>\n      <td>740.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>13.40</td>\n      <td>3.91</td>\n      <td>2.48</td>\n      <td>23.0</td>\n      <td>102.0</td>\n      <td>1.80</td>\n      <td>0.75</td>\n      <td>0.43</td>\n      <td>1.41</td>\n      <td>7.30</td>\n      <td>0.70</td>\n      <td>1.56</td>\n      <td>750.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>13.27</td>\n      <td>4.28</td>\n      <td>2.26</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.59</td>\n      <td>0.69</td>\n      <td>0.43</td>\n      <td>1.35</td>\n      <td>10.20</td>\n      <td>0.59</td>\n      <td>1.56</td>\n      <td>835.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>13.17</td>\n      <td>2.59</td>\n      <td>2.37</td>\n      <td>20.0</td>\n      <td>120.0</td>\n      <td>1.65</td>\n      <td>0.68</td>\n      <td>0.53</td>\n      <td>1.46</td>\n      <td>9.30</td>\n      <td>0.60</td>\n      <td>1.62</td>\n      <td>840.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>14.13</td>\n      <td>4.10</td>\n      <td>2.74</td>\n      <td>24.5</td>\n      <td>96.0</td>\n      <td>2.05</td>\n      <td>0.76</td>\n      <td>0.56</td>\n      <td>1.35</td>\n      <td>9.20</td>\n      <td>0.61</td>\n      <td>1.60</td>\n      <td>560.0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>178 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pd.DataFrame(wine.data, columns=wine.feature_names),pd.DataFrame(wine.target)],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T01:15:44.668965700Z",
     "start_time": "2024-04-16T01:15:44.513660Z"
    }
   },
   "id": "2317266d499ea720",
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
